{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSE 590 Assignment \\#2 (Due 3/1/21)\r\n",
    "A small, but important aspect in text mining and natural language processing is measuring word frequency.  This assignment deals with a heavily boiled-down exercise in loading a text file into Python and computing word frequency statistics.  It requires usage of text files, strings and dataframes, so it is heavily encouraged that you take a look at relevant sessions (14-17) if you have not already done so.\r\n",
    "\r\n",
    "(a) Locate a movie script, play script, poem, or book of your choice in .txt format. You are free to choose nearly any novel, movie script, or play that you like, with the qualification that your chosen document **must have a minimum of 5 chapters, scenes, and/or acts that distinguish one portion of the document's narrative from another**.  For example, the novel \"Great Expectations\" has 59 chapters, the script for \"Jaws\" has about 27 scenes, and all or almost all Shakespearean plays have exactly five acts. It is important that for part (e) of the document that these segments exist for your document. [Project Gutenburg](https://www.gutenberg.org/) is a great resource for this if you're not sure where to start.\r\n",
    "\r\n",
    "(b) Load the words of this structure in sequential order of appearance into a one-dimensional Python list (i.e. the first word should be the first element in the list, while the last word should be the last element) that is **case insensitive**.  It's up to you how to deal with special chacters -- you can remove them manually, ignore them during the loading process, or even count them as words, for example.  **Make sure you have this list clearly assigned to a variable, so we can evaluate it during grading.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "import re\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "wordCounts = {}\r\n",
    "wordListInOrder = []\r\n",
    "ignoreChapterNum = False\r\n",
    "\r\n",
    "#helper function to remove special characters from words\r\n",
    "def filterUnwantedCharacters(word):\r\n",
    "    return re.sub('[!@#$.,?\\';]', '', word)\r\n",
    "\r\n",
    "def addWordOnly(wordList, word):\r\n",
    "    #double dash is Em dash and actually two seperate words\r\n",
    "    if \"--\" in word:\r\n",
    "        hyphenedWords = word.split(\"--\")\r\n",
    "        wordList.append(filterUnwantedCharacters(hyphenedWords[0]))\r\n",
    "        wordList.append(filterUnwantedCharacters(hyphenedWords[1]))\r\n",
    "    else:\r\n",
    "        wordList.append(filterUnwantedCharacters(word))\r\n",
    "\r\n",
    "with open('princessAndGoblin.txt', 'r') as file:\r\n",
    "    for line in file:\r\n",
    "        for word in line.split():\r\n",
    "            #if last word was a chapter, ignore the number\r\n",
    "            if ignoreChapterNum:\r\n",
    "                ignoreChapterNum = False\r\n",
    "                continue\r\n",
    "\r\n",
    "            if word == \"CHAPTER\":\r\n",
    "                ignoreChapterNum = True\r\n",
    "                continue\r\n",
    "\r\n",
    "            addWordOnly(wordListInOrder, word)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Use your list to create **and print** a two-column pandas data-frame with the following properties: <br>\r\n",
    "i. The first column for each index should represent the word in question at that index <br>\r\n",
    "ii. The second column should represent the number of times that particular word appears in the text. <br>\r\n",
    "iii. The rows of the data-frame should be ordered according to the **first** occurrence of each word. <br>\r\n",
    "iv. It's up to you whether or not your data-frame will include an index per row.  <br>**Make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "#convert list to dict for easy counting\r\n",
    "for word in wordListInOrder:\r\n",
    "    if word.lower() in wordCounts:\r\n",
    "        wordCounts[word.lower()][0] += 1\r\n",
    "    else:\r\n",
    "        wordCounts[word.lower()] = [1]\r\n",
    "\r\n",
    "df = pd.DataFrame.from_dict(wordCounts, orient='index')\r\n",
    "df.columns = ['Count']\r\n",
    "\r\n",
    "print(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           Count\n",
      "why          120\n",
      "the         6404\n",
      "princess     518\n",
      "has           58\n",
      "a           2012\n",
      "...          ...\n",
      "skulls         2\n",
      "softer         2\n",
      "degrees        2\n",
      "merciless      2\n",
      "volume         2\n",
      "\n",
      "[4124 rows x 1 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(d) **Stop-words** are commonly used words in a given language that often fail to communicate useful summative information about its content.  The attached stop_words.py file has a simple list of common stop words assigned to a variable.  For this part of the assigment, you are to create a modified copy of the data-frame from (c) with the following modifications: _i. all stop words have been removed from the data-frame_ and _ii. the data frame rows have been sorted in decreasing order of frequency counts_.  **Again, make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "stopWords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"his\",\"her\",\"him\",\"she\",\"they\",\"them\",\"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"i\",\"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]\r\n",
    "dfStopWordsSorted = df.copy()\r\n",
    "\r\n",
    "dfStopWordsSorted.drop(labels=stopWords, axis=0, inplace=True, errors=\"ignore\")\r\n",
    "dfStopWordsSorted = dfStopWordsSorted.sort_values(by='Count', ascending=False)\r\n",
    "dfStopWordsSorted\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>curdie</th>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princess</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irene</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beds</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melted</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>springs</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mossy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3715 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Count\n",
       "curdie      295\n",
       "princess    259\n",
       "irene       171\n",
       "will        164\n",
       "see         142\n",
       "...         ...\n",
       "beds          1\n",
       "melted        1\n",
       "springs       1\n",
       "mossy         1\n",
       "volume        1\n",
       "\n",
       "[3715 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "e) While total word counts can provide a useful measure of the content of a document, they cannot reveal much about its underlying trends.  In the context of document analysis, the term _trend_ implies a direction (in terms of theme, mood, etc.) in which the content changes throughout the narrative.  For example, some works of fiction begin with a comedic tone, and take on a more serious tone in later stages, or vice versa.  For the last part of your assignment, you are going to modify the approach taken in part (d) to address individual segments of the document.  More specifically, you are to divide the raw document into partitions according to the chapters, acts, etc. that are present, and then produce a _list of data-frames_, where **each list element is a single data-frame containing word frequencies for a single segment** with the same format as the data-frame from part (d) outlined above.  You are free to use whatever means you prefer in splitting the text into chapters and constructing the list of data-frames, but one option is to use regular expressions with the raw document.  **Once again, you must insure your list is readily accessible to us in the form of a variable.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "chapterDfList = []\r\n",
    "ignoreChapterNum = False\r\n",
    "chapCount = 0\r\n",
    "\r\n",
    "with open('princessAndGoblin.txt', 'r') as file:\r\n",
    "    text = file.read()\r\n",
    "    text = text.split(\"CHAPTER\")\r\n",
    "    #pop a empty list cause splitting weirdness\r\n",
    "    text.pop(0)\r\n",
    "    for chapter in text:\r\n",
    "        chapterDict = {}\r\n",
    "        chapterWords = []\r\n",
    "        chapCount += 1\r\n",
    "\r\n",
    "        for word in chapter.split():\r\n",
    "            if word.isnumeric():\r\n",
    "                continue\r\n",
    "            addWordOnly(chapterWords, word)\r\n",
    "            \r\n",
    "        #add words in list\r\n",
    "        for word in chapterWords:   \r\n",
    "            if word.lower() in chapterDict:\r\n",
    "                chapterDict[word.lower()][0] += 1\r\n",
    "            else:\r\n",
    "                chapterDict[word.lower()] = [1]\r\n",
    "\r\n",
    "        chapterDf = pd.DataFrame.from_dict(chapterDict, orient='index')\r\n",
    "        chapterDf.columns = ['Count']\r\n",
    "        chapterDf.drop(labels=stopWords, axis=0, inplace=True, errors=\"ignore\")\r\n",
    "        chapterDf = chapterDf.sort_values(by='Count', ascending=False)\r\n",
    "        chapterDfList.append(chapterDf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[           Count\n",
      "princess       5\n",
      "country        5\n",
      "mountains      5\n",
      "other          5\n",
      "people         4\n",
      "...          ...\n",
      "race           1\n",
      "beings         1\n",
      "called         1\n",
      "gnomes         1\n",
      "good           1\n",
      "\n",
      "[213 rows x 1 columns],           Count\n",
      "princess      8\n",
      "doors         7\n",
      "toys          6\n",
      "herself       6\n",
      "stair         5\n",
      "...         ...\n",
      "bed           1\n",
      "cold          1\n",
      "nice          1\n",
      "catch         1\n",
      "creature      1\n",
      "\n",
      "[176 rows x 1 columns],           Count\n",
      "princess     26\n",
      "lady         17\n",
      "will         15\n",
      "see           9\n",
      "eggs          8\n",
      "...         ...\n",
      "carried       1\n",
      "wondered      1\n",
      "straight      1\n",
      "tall          1\n",
      "strange       1\n",
      "\n",
      "[291 rows x 1 columns],              Count\n",
      "nurse           21\n",
      "princess        20\n",
      "will             8\n",
      "grandmother      7\n",
      "never            6\n",
      "...            ...\n",
      "dreamt           1\n",
      "dream            1\n",
      "lost             1\n",
      "should           1\n",
      "asleep           1\n",
      "\n",
      "[208 rows x 1 columns],           Count\n",
      "nurse        10\n",
      "princess      8\n",
      "first         7\n",
      "stair         5\n",
      "lady          4\n",
      "...         ...\n",
      "minded        1\n",
      "occupied      1\n",
      "crossly       1\n",
      "thoughts      1\n",
      "words         1\n",
      "\n",
      "[167 rows x 1 columns],               Count\n",
      "lootie           34\n",
      "nurse            29\n",
      "princess         27\n",
      "dont             22\n",
      "curdie           22\n",
      "...             ...\n",
      "discomposing      1\n",
      "mention           1\n",
      "strict            1\n",
      "servants          1\n",
      "charge            1\n",
      "\n",
      "[540 rows x 1 columns],          Count\n",
      "night       13\n",
      "goblins      9\n",
      "curdie       8\n",
      "work         7\n",
      "father       7\n",
      "...        ...\n",
      "called       1\n",
      "gangs        1\n",
      "lode         1\n",
      "vein         1\n",
      "left         1\n",
      "\n",
      "[291 rows x 1 columns],             Count\n",
      "father         12\n",
      "shoes          11\n",
      "curdie         10\n",
      "feet           10\n",
      "goblin          9\n",
      "...           ...\n",
      "scream          1\n",
      "box             1\n",
      "big             1\n",
      "pray            1\n",
      "reflection      1\n",
      "\n",
      "[458 rows x 1 columns],          Count\n",
      "goblins     20\n",
      "curdie      12\n",
      "thought      8\n",
      "water        8\n",
      "will         7\n",
      "...        ...\n",
      "doorway      1\n",
      "edge         1\n",
      "peering      1\n",
      "quiet        1\n",
      "soundly      1\n",
      "\n",
      "[598 rows x 1 columns],              Count\n",
      "king            14\n",
      "irene           14\n",
      "garden          10\n",
      "great            8\n",
      "sun              7\n",
      "...            ...\n",
      "blue             1\n",
      "nose             1\n",
      "eagle            1\n",
      "dark             1\n",
      "comfortable      1\n",
      "\n",
      "[337 rows x 1 columns],             Count\n",
      "will           16\n",
      "lady           15\n",
      "princess       13\n",
      "irene          13\n",
      "hand           12\n",
      "...           ...\n",
      "lose            1\n",
      "comforting      1\n",
      "sped            1\n",
      "softly          1\n",
      "perfectly       1\n",
      "\n",
      "[437 rows x 1 columns],         Count\n",
      "curdie      8\n",
      "mother      6\n",
      "worked      5\n",
      "string      5\n",
      "good        4\n",
      "...       ...\n",
      "order       1\n",
      "manage      1\n",
      "return      1\n",
      "better      1\n",
      "goblin      1\n",
      "\n",
      "[170 rows x 1 columns],             Count\n",
      "creatures      11\n",
      "goblins         5\n",
      "house           5\n",
      "night           4\n",
      "garden          4\n",
      "...           ...\n",
      "dissonance      1\n",
      "keeping         1\n",
      "shade           1\n",
      "moments         1\n",
      "clear           1\n",
      "\n",
      "[328 rows x 1 columns],           Count\n",
      "irene         8\n",
      "creature      8\n",
      "dark          7\n",
      "light         6\n",
      "long          6\n",
      "...         ...\n",
      "readers       1\n",
      "imagined      1\n",
      "ascents       1\n",
      "pursuing      1\n",
      "ladys         1\n",
      "\n",
      "[298 rows x 1 columns],              Count\n",
      "grandmother     31\n",
      "irene           30\n",
      "lady            21\n",
      "will            15\n",
      "hand            13\n",
      "...            ...\n",
      "laugh            1\n",
      "merry            1\n",
      "spoil            1\n",
      "rain             1\n",
      "dolls            1\n",
      "\n",
      "[469 rows x 1 columns],             Count\n",
      "lootie          6\n",
      "princess        6\n",
      "cat             5\n",
      "nurse           4\n",
      "mountain        4\n",
      "...           ...\n",
      "lost            1\n",
      "yourself        1\n",
      "frightened      1\n",
      "amazement       1\n",
      "king-papa       1\n",
      "\n",
      "[142 rows x 1 columns],            Count\n",
      "irene          6\n",
      "king           5\n",
      "see            4\n",
      "creatures      4\n",
      "asked          3\n",
      "...          ...\n",
      "doesnt         1\n",
      "rings          1\n",
      "answered       1\n",
      "arms           1\n",
      "bones          1\n",
      "\n",
      "[209 rows x 1 columns],           Count\n",
      "curdie       20\n",
      "king         12\n",
      "goblins      10\n",
      "himself       9\n",
      "queen         9\n",
      "...         ...\n",
      "grand         1\n",
      "should        1\n",
      "sentence      1\n",
      "fun           1\n",
      "plight        1\n",
      "\n",
      "[606 rows x 1 columns],               Count\n",
      "queen            21\n",
      "king             13\n",
      "should            8\n",
      "will              8\n",
      "curdie            7\n",
      "...             ...\n",
      "lovely            1\n",
      "housekeepers      1\n",
      "enjoy             1\n",
      "bears             1\n",
      "manufacture       1\n",
      "\n",
      "[346 rows x 1 columns],              Count\n",
      "thread          20\n",
      "path             9\n",
      "thought          6\n",
      "grandmother      6\n",
      "rock             6\n",
      "...            ...\n",
      "leaves           1\n",
      "tiny             1\n",
      "diamond          1\n",
      "ear-rings        1\n",
      "wailing          1\n",
      "\n",
      "[330 rows x 1 columns],           Count\n",
      "curdie       53\n",
      "irene        30\n",
      "thread       29\n",
      "princess     17\n",
      "know         15\n",
      "...         ...\n",
      "high          1\n",
      "others        1\n",
      "parts         1\n",
      "holding       1\n",
      "stair         1\n",
      "\n",
      "[504 rows x 1 columns],              Count\n",
      "curdie          26\n",
      "grandmother     21\n",
      "lady            12\n",
      "see             12\n",
      "irene           12\n",
      "...            ...\n",
      "suppressed       1\n",
      "sobs             1\n",
      "thinks           1\n",
      "turn             1\n",
      "asleep           1\n",
      "\n",
      "[363 rows x 1 columns],             Count\n",
      "mother         33\n",
      "curdie         25\n",
      "will           13\n",
      "know           12\n",
      "dont           11\n",
      "...           ...\n",
      "surrounded      1\n",
      "left-hand       1\n",
      "turn            1\n",
      "sharp           1\n",
      "carry           1\n",
      "\n",
      "[450 rows x 1 columns],           Count\n",
      "princess     15\n",
      "will         11\n",
      "nurse         9\n",
      "lootie        9\n",
      "irene         4\n",
      "...         ...\n",
      "anger         1\n",
      "wild          1\n",
      "half          1\n",
      "screamed      1\n",
      "dress         1\n",
      "\n",
      "[161 rows x 1 columns],           Count\n",
      "curdie       14\n",
      "princess      9\n",
      "night         8\n",
      "good          6\n",
      "time          6\n",
      "...         ...\n",
      "lying         1\n",
      "spot          1\n",
      "knees         1\n",
      "hands         1\n",
      "revisit       1\n",
      "\n",
      "[379 rows x 1 columns],          Count\n",
      "house        6\n",
      "heard        6\n",
      "noises       5\n",
      "rats         4\n",
      "kings        4\n",
      "...        ...\n",
      "strange      1\n",
      "fell         1\n",
      "fast         1\n",
      "asleep       1\n",
      "upwards      1\n",
      "\n",
      "[208 rows x 1 columns],          Count\n",
      "goblins     16\n",
      "curdie      16\n",
      "queen       12\n",
      "house       10\n",
      "rushed       9\n",
      "...        ...\n",
      "alive        1\n",
      "orders       1\n",
      "pieces       1\n",
      "torn         1\n",
      "doom         1\n",
      "\n",
      "[487 rows x 1 columns],             Count\n",
      "curdie         17\n",
      "princess       15\n",
      "mother         14\n",
      "thread          9\n",
      "mountain        7\n",
      "...           ...\n",
      "peacefully      1\n",
      "bed             1\n",
      "opened          1\n",
      "fixed           1\n",
      "darted          1\n",
      "\n",
      "[222 rows x 1 columns],           Count\n",
      "princess      7\n",
      "mother        7\n",
      "curdie        6\n",
      "storm         5\n",
      "mountain      5\n",
      "...         ...\n",
      "hung          1\n",
      "edges         1\n",
      "mist          1\n",
      "sides         1\n",
      "asleep        1\n",
      "\n",
      "[210 rows x 1 columns],           Count\n",
      "king         14\n",
      "curdie       13\n",
      "princess     11\n",
      "face          5\n",
      "lootie        5\n",
      "...         ...\n",
      "returned      1\n",
      "poor          1\n",
      "early         1\n",
      "daylight      1\n",
      "bed           1\n",
      "\n",
      "[223 rows x 1 columns],             Count\n",
      "king           28\n",
      "curdie         28\n",
      "princess       13\n",
      "will            8\n",
      "house           7\n",
      "...           ...\n",
      "fancying        1\n",
      "importance      1\n",
      "upper           1\n",
      "world           1\n",
      "starry          1\n",
      "\n",
      "[322 rows x 1 columns],            Count\n",
      "rest           4\n",
      "goblins        3\n",
      "mountain       3\n",
      "miners         3\n",
      "curdie         3\n",
      "...          ...\n",
      "proceeded      1\n",
      "spoke          1\n",
      "something      1\n",
      "road           1\n",
      "volume         1\n",
      "\n",
      "[93 rows x 1 columns]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "2028cae5e1ccfd73f96864fcab749a741bd0171c015975c3f420b043b6b34333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}