{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSE 590 Assignment \\#2 (Due 3/1/21)\r\n",
    "A small, but important aspect in text mining and natural language processing is measuring word frequency.  This assignment deals with a heavily boiled-down exercise in loading a text file into Python and computing word frequency statistics.  It requires usage of text files, strings and dataframes, so it is heavily encouraged that you take a look at relevant sessions (14-17) if you have not already done so.\r\n",
    "\r\n",
    "(a) Locate a movie script, play script, poem, or book of your choice in .txt format. You are free to choose nearly any novel, movie script, or play that you like, with the qualification that your chosen document **must have a minimum of 5 chapters, scenes, and/or acts that distinguish one portion of the document's narrative from another**.  For example, the novel \"Great Expectations\" has 59 chapters, the script for \"Jaws\" has about 27 scenes, and all or almost all Shakespearean plays have exactly five acts. It is important that for part (e) of the document that these segments exist for your document. [Project Gutenburg](https://www.gutenberg.org/) is a great resource for this if you're not sure where to start.\r\n",
    "\r\n",
    "(b) Load the words of this structure in sequential order of appearance into a one-dimensional Python list (i.e. the first word should be the first element in the list, while the last word should be the last element) that is **case insensitive**.  It's up to you how to deal with special chacters -- you can remove them manually, ignore them during the loading process, or even count them as words, for example.  **Make sure you have this list clearly assigned to a variable, so we can evaluate it during grading.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "import re\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "wordCounts = {}\r\n",
    "wordListInOrder = []\r\n",
    "ignoreChapterNum = False\r\n",
    "\r\n",
    "def filterUnwantedCharacters(word):\r\n",
    "    return re.sub('[!@#$.,?\\';]', '', word)\r\n",
    "\r\n",
    "with open('princessAndGoblin.txt', 'r') as file:\r\n",
    "    for line in file:\r\n",
    "        for word in line.split():\r\n",
    "            #if last word was a chapter, ignore the number\r\n",
    "            if ignoreChapterNum:\r\n",
    "                ignoreChapterNum = False\r\n",
    "                continue\r\n",
    "\r\n",
    "            if word == \"CHAPTER\":\r\n",
    "                ignoreChapterNum = True\r\n",
    "                continue\r\n",
    "\r\n",
    "            #double dash is Em dash and actually two seperate words\r\n",
    "            if \"--\" in word:\r\n",
    "                hyphenedWords = word.split(\"--\")\r\n",
    "                wordListInOrder.append(filterUnwantedCharacters(hyphenedWords[0]))\r\n",
    "                wordListInOrder.append(filterUnwantedCharacters(hyphenedWords[1]))\r\n",
    "            else:\r\n",
    "                wordListInOrder.append(filterUnwantedCharacters(word))\r\n",
    "\r\n",
    "#print(wordListInOrder)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) Use your list to create **and print** a two-column pandas data-frame with the following properties: <br>\r\n",
    "i. The first column for each index should represent the word in question at that index <br>\r\n",
    "ii. The second column should represent the number of times that particular word appears in the text. <br>\r\n",
    "iii. The rows of the data-frame should be ordered according to the **first** occurrence of each word. <br>\r\n",
    "iv. It's up to you whether or not your data-frame will include an index per row.  <br>**Make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "#convert list to dict for easy counting\r\n",
    "for word in wordListInOrder:\r\n",
    "    if word.lower() in wordCounts:\r\n",
    "        wordCounts[word.lower()][0] += 1\r\n",
    "    else:\r\n",
    "        wordCounts[word.lower()] = [1]\r\n",
    "\r\n",
    "df = pd.DataFrame.from_dict(wordCounts, orient='index')\r\n",
    "df.columns = ['Count']\r\n",
    "# print(df)\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>why</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princess</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skulls</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>softer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merciless</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4124 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Count\n",
       "why           60\n",
       "the         3202\n",
       "princess     259\n",
       "has           29\n",
       "a           1006\n",
       "...          ...\n",
       "skulls         1\n",
       "softer         1\n",
       "degrees        1\n",
       "merciless      1\n",
       "volume         1\n",
       "\n",
       "[4124 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(d) **Stop-words** are commonly used words in a given language that often fail to communicate useful summative information about its content.  The attached stop_words.py file has a simple list of common stop words assigned to a variable.  For this part of the assigment, you are to create a modified copy of the data-frame from (c) with the following modifications: _i. all stop words have been removed from the data-frame_ and _ii. the data frame rows have been sorted in decreasing order of frequency counts_.  **Again, make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stopWords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"his\",\"her\",\"him\",\"she\",\"they\",\"them\",\"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"i\",\"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]\r\n",
    "dfStopWordsSorted = df.copy()\r\n",
    "\r\n",
    "dfStopWordsSorted.drop(labels=stopWords, axis=0, inplace=True, errors=\"ignore\")\r\n",
    "dfStopWordsSorted = df.sort_values(by='Count',ascending=False)\r\n",
    "dfStopWordsSorted\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "2028cae5e1ccfd73f96864fcab749a741bd0171c015975c3f420b043b6b34333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}