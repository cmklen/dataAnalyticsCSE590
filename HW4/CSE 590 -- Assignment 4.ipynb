{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSE 590 Assignment 4 (Due 4/27/22)\n",
    "\n",
    "**Note: As with the previous assignment you should submit a separate document (.pdf or .doc(x)) with your responses to the analysis portion of the problems.** \n",
    "\n",
    "**1. (Machine Learning (Classification))** <br>a. Choose one of the [toy classification datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) bundled with sklearn **other than the digits dataset**. <br> b. Train **three** distinct sklearn classification estimators for the chosen dataset and compare the results to see which one performs the best when using **2-fold cross-validation**.  Note that you should use three distinct classification models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use classifiers!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.   <br> c. Repeat a. for **20-fold cross-validation**. Explain in a paragraph the difference in your results when using 20-fold vs 2-fold cross-validation (if any). <br>d. Construct a **confusion matrix** for your _most accurate_ model between the three estimators and two cross-fold options. Which class in your dataset is most accurately predicted to have the correct label by the best classifier, and and which is most likely to be confused among one or more of the wrong classes?\n",
    "\n",
    "**2 (Option I). (Trends, Searches, and Sentiment)** <br>a. Use the Twitter Trends API to determine the available trending topics for a city of your choice, assigning a tweet volume of 5000 to any trend with no volume provided. <br>b. After sorting the trends in descending order by volume, create a bar graph with each (sorted) trend on the x-axis against its volume on the y-axis. <br>c. Use the Twitter Search API to find 20 tweets for each of the three most popular trends in the chosen city, and preprocess their associated tweet text (preferring extended tweet text, if available) in a manner appropriate for tweets. <br>d. Use `TextBlob` to determine the sentiment for each set of 20 tweets.  <br> i. Do you notice a substantial difference in the proportion of positive and negative sentiment for the three trends?  Try to theorize why or why not.  <br> ii. Do you believe the sentiment analysis to be reliable for any or all of the trend?  Explain why or why not.\n",
    "\n",
    "**2 (Option II). (Machine Learning (Regression))** <br>a. Locate a non-proprietary, small-scale dataset _suitable for regression_ online.  There are countless sources and repositories than you can use in this task, but if you have trouble finding one, I recommend starting via Kaggle (https://www.kaggle.com/code/rtatman/datasets-for-regression-analysis/notebook).  Explain briefly what the dataset represents, what target variable you will be using, and what other features are present.  _You may want or need to apply preprocessing to your data to insure it can be used properly with the regression models_ (e.g. making every feature numeric through transformation or by dropping some)  <br> b. Train **three** distinct sklearn regression estimators for the chosen dataset and compare the results to see which one performs the best when using **10-fold cross-validation**, utilizing the R-Squared score to gauge performance.  Note that you should use two distinct regression models here (not just tweak underlying parameters).  A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use regression models!  Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.<br>  c. Repeat part b utilizing the Mean Square Error to gauge performance.  _Briefly_ research the difference between the two metrics (MSE and R2), and explain in a paragraph or two i. the difference between them ii. when each one is the preferable metric to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
